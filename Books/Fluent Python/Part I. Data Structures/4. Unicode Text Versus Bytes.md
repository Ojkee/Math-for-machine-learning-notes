- We started the chapter by dismissing the notion that ***1 character == 1 byte***. As the world adopts Unicode, we need to keep the concept of text strings separated from the binary sequences that represent them in files, and Python 3 enforces this separation.

```python
>>> s = 'café'
>>> len(s)
4

>>> b = s.encode('utf8')
>>> b 
b'caf\xc3\xa9'

>>> len(b)
5

>>> b.decode('utf8')
'café'
```
 
--------------------

- After a brief overview of the binary sequence data types—`bytes`, `bytearray`, and `memoryview`—we jumped into encoding and decoding, with a sampling of important codecs, followed by approaches to prevent or deal with the infamous `UnicodeEncodeError`, `UnicodeDecodeError`, and the `SyntaxError` caused by wrong encoding in Python source files.

```python
>>> cafe = bytes('café', encoding='utf_8')
>>> cafe 
b'caf\xc3\xa9'

>> cafe[0]
99
>> cafe[:1]
b'c'

>>> cafe_arr = bytearray(cafe)
>>> cafe_arr
bytearray(b'caf\xc3\xa9')

>>> cafe_arr[-1:]
bytearray(b'\xa9')
```

```python
>>> bytes.fromhex('32 4B CE A9')
b'1K\xce\xa9'
```

```python
>>> import array
>>> numbers = array.array('h', [-2, -1, 0, 1, 2])
>>> octets = bytes(numbers)
>>> octets
b'\xfe\xff\xff\xff\x00\x00\x01\x00\x02\x00'
```

```python
>>> for codec in ['latin_1', 'utf_8', 'utf_16']:
>>> 	pri8nt(codec, 'El Niño'.encode(codec), sep='\t')
latin_1 b'El Ni\xf1o'
utf_8   b'El Ni\xc3\xb1o'
utf_16  b'\xff\xfeE\x00l\x00 \x00N\x00i\x00\xf1\x00o\x00'
```

```python
>>> city = 'São Paulo'
>>> city.encode('utf_8')
b'S\xc3\xa3o Paulo'

>>> city.encode('utf_16')
b'\xff\xfeS\x00\xe3\x00o\x00 \x00P\x00a\x00u\x00l\x00o\x00'

>>> city.encode('iso8859_1')
b'S\xe3o Paulo'

>>> city.encode('cp437')
Traceback (most recent call last):
	File "<stdin>", line 1, in <module>
	File "/.../lib/python3.4/encodings/cp437.py", line 12, in encode
		return codecs.charmap_encode(input,errors,encoding_map)
UnicodeEncodeError: 'charmap' codec can't encode character '\xe3' in
position 1: character maps to <undefined>

>>> city.encode('cp437', errors='ignore')
b'So Paulo'

>>> city.encode('cp437', errors='replace')
b'S?o Paulo'

>>> city.encode('cp437', errors='xmlcharrefreplace')
b'S&#227;o Paulo'
```

```python
>>> octets = b'Montr\xe9al'
>>> octets.decode('cp1252')
'Montréal'

>>> octets.decode('iso8859_7')
'Montrιal'

>>> octets.decode('koi8_r')
'MontrИal'

>>> octets.decode('utf_8')
Traceback (most recent call last):
	File "<stdin>", line 1, in <module>
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe9 in position 5:
invalid continuation byte

>>> octets.decode('utf_8', errors='replace')
'Montr�al'
```

```python
# coding: cp1252

print('Olá, Mundo!')
```

--------------------

- We then considered the theory and practice of encoding detection in the absence of metadata: in theory, it can’t be done, but in practice the `Chardet` package pulls it off pretty well for a number of popular encodings. Byte order marks (BOM) were then presented as the only encoding hint commonly found in UTF-16 and UTF-32 files—sometimes in UTF-8 files as well.

```python
>>> u16 = 'El Niño'.encode('utf_16')
>>> u16
b'\xff\xfeE\x00l\x00 \x00N\x00i\x00\xf1\x00o\x00'
```

```python
>>> u16le = 'El Niño'.encode('utf_16le')
>>> list(u16le)
[69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111, 0]

>>> u16be = 'El Niño'.encode('utf_16be')
>>> list(u16be)
[0, 69, 0, 108, 0, 32, 0, 78, 0, 105, 0, 241, 0, 111]
```

--------------------

- In the next section, we demonstrated opening text files, an easy task except for one pitfall: the `encoding=` keyword argument is not mandatory when you open a text file, but it should be. If you fail to specify the encoding, you end up with a program that manages to generate “plain text” that is incompatible across platforms, due to conflicting default encodings. We then exposed the different encoding settings that Python uses as defaults and how to detect them. A sad realization for Windows users is that these settings often have distinct values within the same machine, and the values are mutually incompatible; GNU/Linux and macOS users, in contrast, live in a happier place where UTF-8 is the default pretty much everywhere.

![[Pasted image 20251003184304.png]]

```python
>>> open('cafe.txt', 'w', encoding='utf_8').write('café')
4

>>> open('cafe.txt').read()
'cafÃ©'
```

```python
>>> fp = open('cafe.txt', 'w', encoding='utf_8')
>>> fp
<_io.TextIOWrapper name='cafe.txt' mode='w' encoding='utf_8'>

>>> fp.write('café')
4

>>> fp.close()

>>> import os
>>> os.stat('cafe.txt').st_size
5

>>> fp2 = open('cafe.txt')
>>> fp2
<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='cp1252'>

>>> fp2.encoding
'cp1252'

>>> fp2.read()
'cafÃ©'

>>> fp3 = open('cafe.txt', encoding='utf_8')
>>> fp3
<_io.TextIOWrapper name='cafe.txt' mode='r' encoding='utf_8'>

>>> fp3.read()
'café'

>>> fp4 = open('cafe.txt', 'rb')
>>> fp4
<_io.BufferedReader name='cafe.txt'>

>>> fp4.read()
b'caf\xc3\xa9'
```

```python
import sys
from unicodedata import name
print(sys.version)
print()
print('sys.stdout.isatty():', sys.stdout.isatty())
print('sys.stdout.encoding:', sys.stdout.encoding)
print()

test_chars = [
	'\N{HORIZONTAL ELLIPSIS}',      # exists in cp1252, not in cp437
	'\N{INFINITY}',                 # exists in cp437, not in cp1252
	'\N{CIRCLED NUMBER FORTY TWO}', # not in cp437 or in cp1252
]

for char in test_chars:
	print(f'Trying to output {name(char)}:')
	print(char)
```

![[Pasted image 20251003184545.png]]

--------------------

- Unicode provides multiple ways of representing some characters, so normalizing is a prerequisite for text matching. In addition to explaining normalization and case folding, we presented some utility functions that you may adapt to your needs, including drastic transformations like removing all accents. We then saw how to sort Unicode text correctly by leveraging the standard locale module—with some caveats—and an alternative that does not depend on tricky locale configurations: the external `pyuca` package.

```python
>>> s1 = 'café'
>>> s2 = 'cafe\N{COMBINING ACUTE ACCENT}'
>>> s1, s2
('café', 'café')

>>> len(s1), len(s2)
(4, 5)

>>> s1 == s2
False
```

```python
>>> from unicodedata import normalize
>>> s1 = 'café'
>>> s2 = 'cafe\N{COMBINING ACUTE ACCENT}'
>>> len(s1), len(s2)
(4, 5)

>>> len(normalize('NFC', s1)), len(normalize('NFC', s2))
(4, 4)

>>> len(normalize('NFD', s1)), len(normalize('NFD', s2))
(5, 5)

>>> normalize('NFC', s1) == normalize('NFC', s2)
True

>>> normalize('NFD', s1) == normalize('NFD', s2)
True
```

```python
>>> from unicodedata import normalize, name
>>> ohm = '\u2126'
>>> name(ohm)
'OHM SIGN'

>>> ohm_c = normalize('NFC', ohm)
>>> name(ohm_c)
'GREEK CAPITAL LETTER OMEGA'

>>> ohm == ohm_c
False

>>> normalize('NFC', ohm) == normalize('NFC', ohm_c)
True
```

```python
>>> from unicodedata import normalize, name
>>> half = '\N{VULGAR FRACTION ONE HALF}'
>>> print(half)
½

>>> normalize('NFKC', half)
'1⁄2'

>>> for char in normalize('NFKC', half):
>>> 	print(char, name(char), sep='\t')
1 DIGIT ONE
⁄ FRACTION SLASH
2 DIGIT TWO

>>> four_squared = '4²'
>>> normalize('NFKC', four_squared)
'42'

>>> micro = 'µ'
>>> micro_kc = normalize('NFKC', micro)
>>> micro, micro_kc
('µ', 'μ')

>>> ord(micro), ord(micro_kc)
(181, 956)

>>> name(micro), name(micro_kc)
('MICRO SIGN', 'GREEK SMALL LETTER MU')
```

```python
>>> micro = 'µ'
>>> name(micro)
'MICRO SIGN'

>>> micro_cf = micro.casefold()
>>> name(micro_cf)
'GREEK SMALL LETTER MU'

>>> micro, micro_cf
('µ', 'μ')

>>> eszett = 'ß'
>>> name(eszett)
'LATIN SMALL LETTER SHARP S'

>>> eszett_cf = eszett.casefold()
>>> eszett, eszett_cf
('ß', 'ss')
```

```python
from unicodedata import normalize

def nfc_equal(str1, str2):
	return normalize('NFC', str1) == normalize('NFC', str2)
	
def fold_equal(str1, str2):
	return (normalize('NFC', str1).casefold() ==
			normalize('NFC', str2).casefold())
```

```python
import unicodedata
import string

def shave_marks(txt):
	"""Remove all diacritic marks"""
	norm_txt = unicodedata.normalize('NFD', txt)
	shaved = ''.join(c for c in norm_txt
					 if not unicodedata.combining(c))
	return unicodedata.normalize('NFC', shaved)

>>> order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'
>>> shave_marks(order)
'“Herr Voß: • ½ cup of Œtker™ caffe latte • bowl of acai.”'

>>> Greek = 'Ζέφυρος, Zéfiro'
>>> shave_marks(Greek)
'Ζεφυρος, Zefiro'
```

```python
single_map = str.maketrans("""‚ƒ„ˆ‹‘’“”•–—˜›""",
						   """'f"^<''""---~>""")

multi_map = str.maketrans({
	'€': 'EUR',
	'…': '...',
	'Æ': 'AE',
	'æ': 'ae',
	'Œ': 'OE',
	'œ': 'oe',
	'‰': '<per mille>',
	'†': '**',
	'‡': '***',
})

multi_map.update(single_map)
def dewinize(txt):
	"""Replace Win1252 symbols with ASCII chars or sequences"""
	return txt.translate(multi_map)

def asciize(txt):
	no_marks = shave_marks_latin(dewinize(txt))
	no_marks = no_marks.replace('ß', 'ss')
	return unicodedata.normalize('NFKC', no_marks)

>>> order = '“Herr Voß: • ½ cup of Œtker™ caffè latte • bowl of açaí.”'
>>> dewinize(order)
'"Herr Voß: - ½ cup of OEtker(TM) caffè latte - bowl of açaí."'

>>> asciize(order)
'"Herr Voss: - 1⁄2 cup of OEtker(TM) caffe latte - bowl of acai."'
```

```python
>>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']
>>> sorted(fruits)
['acerola', 'atemoia', 'açaí', 'caju', 'cajá']
```

```python
>>> import locale
>>> my_locale = locale.setlocale(locale.LC_COLLATE, 'pt_BR.UTF-8')
>>> print(my_locale)
'pt_BR.UTF-8'

>>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']
>>> sorted_fruits = sorted(fruits, key=locale.strxfrm)
>>> print(sorted_fruits)
['açaí', 'acerola', 'atemoia', 'cajá', 'caju']
```

```python
>>> import pyuca
>>> coll = pyuca.Collator()
>>> fruits = ['caju', 'atemoia', 'cajá', 'açaí', 'acerola']
>>> sorted_fruits = sorted(fruits, key=coll.sort_key)
>>> sorted_fruits
['açaí', 'acerola', 'atemoia', 'cajá', 'caju']
```

--------------------

- We leveraged the Unicode database to program a command-line utility to search for characters by name—in 28 lines of code, thanks to the power of Python. We glanced at other Unicode metadata, and had a brief overview of dual-mode APIs where some functions can be called with `str` or `bytes` arguments, producing different results.

```python
#!/usr/bin/env python3
import sys
import unicodedata

START, END = ord(' '), sys.maxunicode + 1
def find(*query_words, start=START, end=END):
	query = {w.upper() for w in query_words}
	for code in range(start, end):
		char = chr(code)
		name = unicodedata.name(char, None)
		if name and query.issubset(name.split()):
			print(f'U+{code:04X}\t{char}\t{name}')
			
def main(words):
	if words:
		find(*words)
	else:
		print('Please provide words to find.')

if __name__ == '__main__':
	main(sys.argv[1:])
```

```python
import unicodedata
import re

re_digit = re.compile(r'\d')
sample = '1\xbc\xb2\u0969\u136b\u216b\u2466\u2480\u3285'

for char in sample:
	print(f'U+{ord(char):04x}',
		  char.center(6),
		  're_dig' if re_digit.match(char) else '-',
		  'isdig' if char.isdigit() else '-',
		  'isnum' if char.isnumeric() else '-',
		  f'{unicodedata.numeric(char):5.2f}',
		  unicodedata.name(char),
		  sep='\t')
```

![[Pasted image 20251003185618.png]]

```python
import re

re_numbers_str = re.compile(r'\d+')
re_words_str = re.compile(r'\w+')
re_numbers_bytes = re.compile(rb'\d+')
re_words_bytes = re.compile(rb'\w+')

text_str = ("Ramanujan saw \u0be7\u0bed\u0be8\u0bef"
			" as 1729 = 1³ + 12³ = 9³ + 10³.")
text_bytes = text_str.encode('utf_8')

print(f'Text\n {text_str!r}')
print('Numbers')
print(' str :', re_numbers_str.findall(text_str))
print(' bytes:', re_numbers_bytes.findall(text_bytes))
print('Words')
print(' str :', re_words_str.findall(text_str))
print(' bytes:', re_words_bytes.findall(text_bytes))
```

![[Pasted image 20251003185704.png]]

```python
>>> os.listdir('.')
['abc.txt', 'digits-of-π.txt']

>>> os.listdir(b'.')
[b'abc.txt', b'digits-of-\xcf\x80.txt']
```

--------------------
